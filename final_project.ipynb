{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "desperate-tongue",
      "metadata": {
        "id": "desperate-tongue"
      },
      "source": [
        "# Final Project: Analysis of GW200129\n",
        "\n",
        "For this assignment, you will have to conduct an analysis of the gravitational-wave signal GW200129_065458 (thereafter simply GW200129). This analysis should be done within a Jupyter notebook, with many comments in [Markdown](https://colab.research.google.com/notebooks/markdown_guide.ipynb) desribing and **justifying** the steps taken.\n",
        "\n",
        "- The writing should be at the level of a fellow student of the class\n",
        "- Describe **in details** every step of your analysis throughout the notebook.\n",
        "- Any setting choice (sampling rate, window parameter, FFT length, etc...) needs to be justified, either by its own investigation or relevant citations from the literature.\n",
        "- Code alone, without a detailed descriptions of what is done and why, is worth at most half marks.\n",
        "- Remember to re-run your notebook from scratch after a kernel re-start before submission.\n",
        "\n",
        "NOTE: you can use any of the notebooks covered in the class, or code available online, _provided that you justify the settings and methods used_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "saved-world",
      "metadata": {
        "id": "saved-world"
      },
      "source": [
        "## Introduction (15 points)\n",
        "Minimum 300 words.\n",
        "- Write an introduction to this analysis, citing the relevant context and literature.\n",
        "- Write a short summary of what is done in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GW200129 [1], a gravitational wave event detected during LIGO-Virgo's third observational run (O3), presented challenges in parameter inference due to data quality issues. The event’s high signal-to-noise ratio (SNR) led to large discrepancies between various CBC models. Spin-precession inference was particularly difficult because of variations across detectors and different pipelines, with glitches in LIGO’s data exacerbating the problem. Additionally, the Virgo detector reported a low SNR, resulting in an inaccurate mass estimate. This analysis aims to parametrize GW200129 using both coherent and coincident methods, focusing on the impact of the glitch and comparing different CBC models, including SEOBNRv4_opt, in contrast to NRSur7dq4 and IMRPhenomXPHM. The study emphasizes that events like GW200129, dominated by the merger phase and high masses, require careful consideration of both waveform systematics and potential small data quality issues, as these can affect subtle parameter inferences.\n",
        "\n",
        "In the original analysis of GW200129, different gravitational waveform models were employed to capture the binary coalescence (CBC) dynamics and glitches. The study utilized CBC models alongside a glitch model to characterize the observed waveforms, with post-hoc waveform generation performed by BayesWave [2]. For parameter estimation, the analysis relied on the Bilby framework, which uses Markov Chain Monte Carlo (MCMC) samplers to derive posterior distributions of model parameters. In our approach, we use the PyCBC [3] framework for CBC waveform analysis. The Powell method is utilized as an optimizer for efficient parameter space exploration, while MCMC is again employed for parameter estimation, ensuring a robust exploration of the parameter space. Additionally, a burst model was considered for specific waveform features, further informing the analysis of the signal.\n",
        "\n",
        "Initially this project will do an initial exploration of the data by plotting both the time series and PSD representations of the signal received at both L1 and H1. The data received at H1 was then matched filtered 'by eye' using two different methods: a waveform generated with the SEOBNRv4_opt CBC model; and a sine-Gaussian. Following this a true coincident match-filter analysis with both models by computing the SNR values of each model. This was done on the H1 data, which had the highest SNR of all detections of ~25. A coincident analysis was then performed with H1 and L1, which allowed for a Bayesian analysis to be performed. The was intitially done with an optimisation of the posterior using the Powell method, generating maximum a postiori parameters. Following this, a true posterior sampling was performed using an MCMC, which allowed for a corner plot of both correlation distribution between each and a posterior distribution of each parameter."
      ],
      "metadata": {
        "id": "E-l6wBzoj9T1"
      },
      "id": "E-l6wBzoj9T1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "supreme-polls",
      "metadata": {
        "id": "supreme-polls"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ecological-pakistan",
      "metadata": {
        "id": "ecological-pakistan"
      },
      "source": [
        "## The data and noise model (10 points)\n",
        "\n",
        "- Download and plot the relevant gravitational-wave data for GW200129.\n",
        "- Compute and plot the Power Spectrum Density.\n",
        "- Compute and plot the whitened data. Use filters if necessary to best highlight where the signal is (or might be)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "guilty-cassette",
      "metadata": {
        "id": "guilty-cassette"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gwpy"
      ],
      "metadata": {
        "id": "uYOptmmbFM1M",
        "outputId": "d2fff4eb-f5af-46eb-fe75-31c1b07f2dfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uYOptmmbFM1M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q lalsuite pycbc"
      ],
      "metadata": {
        "id": "hlGA3NFPFOAO"
      },
      "id": "hlGA3NFPFOAO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import get_window"
      ],
      "metadata": {
        "id": "eISrqA74FQbG"
      },
      "id": "eISrqA74FQbG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gwosc.datasets import event_gps"
      ],
      "metadata": {
        "id": "ySmA8fJfFRwa"
      },
      "id": "ySmA8fJfFRwa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gwpy.timeseries import TimeSeries"
      ],
      "metadata": {
        "id": "qH7h0hxuFTFg"
      },
      "id": "qH7h0hxuFTFg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_centre = event_gps('GW200129_065458')\n",
        "\n",
        "print(time_centre)"
      ],
      "metadata": {
        "id": "jM3MVJLjFYNG"
      },
      "id": "jM3MVJLjFYNG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GW_200129_strain_H1 = TimeSeries.fetch_open_data('H1', start, end) #fetches the time series from H1\n",
        "GW_200129_strain_L1 = TimeSeries.fetch_open_data('L1', start, end) #fetches the time series from L1"
      ],
      "metadata": {
        "id": "Kt4zRnC4FeHe"
      },
      "id": "Kt4zRnC4FeHe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H1_strain = GW_200129_strain_H1.plot()"
      ],
      "metadata": {
        "id": "52vDhqxiFfrH"
      },
      "id": "52vDhqxiFfrH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L1_strain = GW_200129_strain_L1.plot()"
      ],
      "metadata": {
        "id": "lMjzDIIYFhL3"
      },
      "id": "lMjzDIIYFhL3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalised_fft=GW_200129_strain_H1.average_fft(window=('tukey',1./4.))*(GW_200129_strain_H1.duration.value/2)"
      ],
      "metadata": {
        "id": "VR5Dt2lsFjB1"
      },
      "id": "VR5Dt2lsFjB1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.loglog((2/GW_200129_strain_H1.duration.value)*np.abs(normalised_fft)**2,label='manual␣PSD')\n",
        "plt.loglog(GW_200129_strain_H1.psd(window=('tukey',1./4.)),label='Built-in PSD')\n",
        "plt.ylabel('Strain noise [$1/\\mathrm{Hz}$]')\n",
        "plt.xlabel('Frequency [Hz]')\n",
        "plt.xlim(10,1024)\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "zmonytc2Fkr8"
      },
      "id": "zmonytc2Fkr8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "strain_H1_white = GW_200129_strain_H1.whiten(fftlength=4,overlap=2,window=('tukey',1./4.))\n",
        "# Getting a more manageable 32 seconds of data:\n",
        "strain_white=strain_H1_white.crop(time_centre - 16,time_centre+ 16)\n"
      ],
      "metadata": {
        "id": "5DASjI37Fn88"
      },
      "id": "5DASjI37Fn88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(strain_white)\n",
        "#plt.xlim(gps+0.25,gps+0.46)\n",
        "plt.ylabel('H1 strain')\n",
        "plt.xlabel('time [s]')"
      ],
      "metadata": {
        "id": "9FWT_i96Fsju"
      },
      "id": "9FWT_i96Fsju",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'The rate of glitches at frequencies similar to the signal is much lower; using data from 4 days around the event, the rate of glitches with frequency 60-120 Hz is only 0.06/hr.'"
      ],
      "metadata": {
        "id": "cqA4HKvkF3WH"
      },
      "id": "cqA4HKvkF3WH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Badpassing the whitened strain with specific values are defined in the detection paper (will look up lol)\n",
        "strain_bp=strain_white.bandpass(60.,120.)"
      ],
      "metadata": {
        "id": "dOnkrkT-F5-0"
      },
      "id": "dOnkrkT-F5-0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.plot(strain_bp)\n",
        "#plt.xlim(gps+0.15,gps-0.05)\n",
        "plt.ylabel('H1 strain')\n",
        "plt.xlabel('time [s]')\n",
        "\n"
      ],
      "metadata": {
        "id": "n-7QVMBDF7XD"
      },
      "id": "n-7QVMBDF7XD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Obvious peak present after band passing"
      ],
      "metadata": {
        "id": "suPQnezCF-XB"
      },
      "id": "suPQnezCF-XB"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(strain_bp)\n",
        "plt.xlim(time_centre-0.15,time_centre+0.05)\n",
        "plt.ylabel('H1 strain')\n",
        "plt.xlabel('time [s]')"
      ],
      "metadata": {
        "id": "jMyS4vUrGBgA"
      },
      "id": "jMyS4vUrGBgA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "synthetic-vatican",
      "metadata": {
        "id": "synthetic-vatican"
      },
      "source": [
        "## The signal model (10 points)\n",
        "\n",
        "- Generate and plot a CBC signal which you expect to be a good match for the signal. Use the literature, and cite your sources\n",
        "- Generate and plot a phenomenological model (sometimes refer to as a \"Burst\" model). A model whose parameters describe the shape of the waveform itself, not the source.\n",
        "- Plot those those two signals together, and try to find _by eye_ parameters that maximises that overlap. Discuss.\n",
        "\n",
        "NOTE: remember that gravitational-wave are redshifted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "persistent-avenue",
      "metadata": {
        "id": "persistent-avenue"
      },
      "outputs": [],
      "source": [
        "import pycbc\n",
        "from pycbc.waveform import get_td_waveform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strain_bp=strain_white.bandpass(60.,120.).crop(time_centre-0.15,time_centre+0.05)"
      ],
      "metadata": {
        "id": "pkHybGksGHGf"
      },
      "id": "pkHybGksGHGf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "I could definitely add distance\n",
        "'''\n",
        "m1 = 34.5 * (1 + 0.18)\n",
        "m2 = 29 * (1 + 0.18)\n",
        "hp, hc = get_td_waveform(approximant=\"SEOBNRv4_opt\",\n",
        "mass1=m1,\n",
        "mass2=m2,\n",
        "delta_t=1./GW_200129_strain_H1.sample_rate.to_value(),\n",
        "f_lower=20)\n",
        "\n",
        "plt.plot(hp.sample_times, hp)\n",
        "plt.xlabel('Time (s)');\n",
        "\n",
        "#hp.start_time = gps + 0.01"
      ],
      "metadata": {
        "id": "L7asuBLpGIrB"
      },
      "id": "L7asuBLpGIrB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pycbc.waveform import get_td_waveform\n",
        "\n",
        "# Define masses with correction factor\n",
        "m1 = 34.5 * (1 + 0.18)\n",
        "m2 = 29 * (1 + 0.18)\n",
        "\n",
        "# Define sample rate (ensure GW_200129_strain_H1.sample_rate is defined)\n",
        "delta_t = 1. / GW_200129_strain_H1.sample_rate.to_value()\n",
        "\n",
        "# Generate waveform\n",
        "hp, hc = get_td_waveform(approximant=\"SEOBNRv4PHM\",\n",
        "                          mass1=m1,\n",
        "                          mass2=m2,\n",
        "                          delta_t=delta_t,\n",
        "                          spin1x=0.9,\n",
        "                          f_lower=20)\n",
        "\n",
        "# Find the current peak strain (you could adjust this to your expected peak)\n",
        "# Convert hp to a NumPy array before calling np.max\n",
        "current_peak = np.max(np.abs(hp.numpy()))\n",
        "\n",
        "\n",
        "# Define the desired peak strain (e.g., we want it to be 1)\n",
        "desired_peak = 1.0\n",
        "\n",
        "# Calculate the scaling factor\n",
        "scaling_factor = desired_peak / current_peak\n",
        "\n",
        "# Apply the scaling factor to the waveform\n",
        "hp_scaled = hp * scaling_factor\n",
        "\n",
        "# Plot the scaled waveform\n",
        "plt.plot(hp.sample_times, hp_scaled)\n",
        "\n",
        "hp.start_time = time_centre - 0.545"
      ],
      "metadata": {
        "id": "qQ8DjWAfGLsA"
      },
      "id": "qQ8DjWAfGLsA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(strain_bp.times,strain_bp,label='Filtered H1 data', color='gwpy:ligo-hanford')\n",
        "#plt.plot(hp.sample_times,hp,label='template')\n",
        "plt.plot(hp.sample_times, hp_scaled, label='Template waveform', color='black', linewidth=1.2)\n",
        "plt.xlabel('GPS Time [s]')\n",
        "plt.ylabel('Strain')\n",
        "plt.xlim(time_centre-0.15,time_centre+0.05)\n",
        "#plt.ylim(-5e-22, 5e-22)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "eLVNYgYlGOw4"
      },
      "id": "eLVNYgYlGOw4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Above is a waveform I've scaled up to fit to the data. Idk if I'm even suppsoed to do this\n"
      ],
      "metadata": {
        "id": "btYPzmuwGRKV"
      },
      "id": "btYPzmuwGRKV"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Waveform duration: {hp.duration:.3f} s\")"
      ],
      "metadata": {
        "id": "lYn13qF-GSxQ"
      },
      "id": "lYn13qF-GSxQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import gausspulse"
      ],
      "metadata": {
        "id": "omCP422nGUk3"
      },
      "id": "omCP422nGUk3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = hp.sample_times[np.argmax(np.abs(hp))] # gives time hp reaches maximum value"
      ],
      "metadata": {
        "id": "sXQh6rxUGV5J"
      },
      "id": "sXQh6rxUGV5J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_shifted = hp.sample_times - t0 #shifts gaussian to the right time"
      ],
      "metadata": {
        "id": "H7sTrY52GXOf"
      },
      "id": "H7sTrY52GXOf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(time_shifted, gausspulse(time_shifted + 0.005, fc=95, bw=0.35)*7.6e-19, label='Gaussian burst')\n",
        "plt.plot(time_shifted, hp, label='Template waveform')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.legend()\n",
        "plt.xlim(-0.1, 0.1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EmXZNYZCGYsn"
      },
      "id": "EmXZNYZCGYsn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "reported-milton",
      "metadata": {
        "id": "reported-milton"
      },
      "source": [
        "## Detection (25 points)\n",
        "Limiting yourself to a coincident (**not** coherent) analysis:\n",
        "- Compute the SNR time series for the CBC signal model\n",
        "- Compute the SNR time series for the Burst signal model\n",
        "- Discuss the differences, if any.\n",
        "- Estimate a bound on the False Alarm Rate of the detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "legendary-alberta",
      "metadata": {
        "id": "legendary-alberta"
      },
      "outputs": [],
      "source": [
        "\n",
        "strain_H1 = TimeSeries.fetch_open_data('H1', time_centre - 16, time_centre +16)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strain is limited to 16 seconds are the time centre of the signal."
      ],
      "metadata": {
        "id": "FiR1fTpvHfpg"
      },
      "id": "FiR1fTpvHfpg"
    },
    {
      "cell_type": "code",
      "source": [
        "Pxx_H1=strain_H1.psd(fftlength=4.,window=('tukey',1./4.),method='welch',overlap=2.)"
      ],
      "metadata": {
        "id": "UnZ_k8XIHkZ5"
      },
      "id": "UnZ_k8XIHkZ5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the fast fourier transformer to calculate the PSD of the signal, four hyperparameters were selected.\n",
        "\n",
        "A Fast Fourier Transform (FFT) length of 4 seconds results in a frequency resolution of 0.25 Hz, meaning the power spectral density (PSD) is computed in discrete bins spaced 0.25 Hz apart. This allows for the detection of small frequency variations while keeping computational efficiency manageable. Spectral leakage can occur when signal frequencies do not align perfectly with these bins, causing energy to spread into neighboring bins. To mitigate this, a Tukey window is applied.\n",
        "\n",
        "The Tukey window reduces spectral leakage by tapering the edges of each segment before applying the FFT. The tapering is controlled by the shape parameter (1/4), which determines the balance between a Hann-like window (stronger tapering, less leakage) and a rectangular window (no tapering, sharper spectral resolution but more leakage). A value of 1/4 provides a good trade-off between reducing spectral leakage and maintaining frequency resolution.\n",
        "\n",
        "The overlap setting controls how much adjacent FFT segments overlap in time when using Welch’s method to estimate the PSD. A higher overlap smooths the PSD by reducing statistical variance, improving stability without affecting frequency resolution.\n",
        "\n",
        "The high sample rate of 4096Hz gives a Nyquist frequency of 2048Hz, which is the highest frequency we're able to analyse. This allows for a high fft length of 4, which gives a good enough spectral resolution to detec"
      ],
      "metadata": {
        "id": "ipRD3XqOHp6P"
      },
      "id": "ipRD3XqOHp6P"
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = 34.5*1.18 # Solar masses\n",
        "m2 = 28.9*1.18 # Solar masses\n",
        "\n",
        "h_plus, _ = get_td_waveform(approximant=\"SEOBNRv4_opt\",\n",
        "                     mass1=m1,\n",
        "                     mass2=m2,\n",
        "                     delta_t=strain_H1.dt.value,\n",
        "                     f_lower=20)\n",
        "\n",
        "plt.plot(h_plus.sample_times, h_plus,label='Before Taper')\n",
        "plt.plot(h_plus.sample_times, h_plus*get_window(('tukey',1/4),h_plus.shape[0]),label='After Taper')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "v2zYhtAKHtb0"
      },
      "id": "v2zYhtAKHtb0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tukey window was also applied to the generated waveform. This is important for calculating the correct SNR.:"
      ],
      "metadata": {
        "id": "GXB9HR3ZHwXY"
      },
      "id": "GXB9HR3ZHwXY"
    },
    {
      "cell_type": "code",
      "source": [
        "strain_white = strain_H1.whiten(fftlength=4,overlap=2,window=('tukey',1./4.))"
      ],
      "metadata": {
        "id": "9XiqkKAqHyuX"
      },
      "id": "9XiqkKAqHyuX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The strain is then whitened using the same hyperparameters as the PSD calculation, since it also requires a calculation of the PSD to be computed. Whitening flattens the spectrum by dividing the signal (which has been fourier transformed) by the square root of the PSD. This step is important by making all frequencies equally weighted, which is important for identifying weaker signals."
      ],
      "metadata": {
        "id": "OCtp9XTMH4cb"
      },
      "id": "OCtp9XTMH4cb"
    },
    {
      "cell_type": "code",
      "source": [
        "#Badpassing the whitened strain with specific values are defined in the detection paper\n",
        "strain_bp=strain_white.bandpass(60.,120.)"
      ],
      "metadata": {
        "id": "haK9lWfcH8g_"
      },
      "id": "haK9lWfcH8g_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(strain_bp)\n",
        "plt.xlim(time_centre-0.2,time_centre+0.2)\n",
        "plt.ylabel('H1 strain')\n",
        "plt.xlabel('time [s]')"
      ],
      "metadata": {
        "id": "U6jIFWKIH-3k"
      },
      "id": "U6jIFWKIH-3k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making it 32 seconds:\n",
        "h_plus.prepend_zeros(np.ceil((16+h_plus.start_time)/h_plus.delta_t))\n",
        "h_plus.append_zeros(np.floor((16-h_plus.end_time)/h_plus.delta_t))\n",
        "\n",
        "template=TimeSeries.from_pycbc(h_plus)\n",
        "template.duration"
      ],
      "metadata": {
        "id": "q5_0v_YqIA3i"
      },
      "id": "q5_0v_YqIA3i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated waveform is padded with zeros to allow it to be 32 seconds, which is the length of the cropped data."
      ],
      "metadata": {
        "id": "_HmX9cmauv2d"
      },
      "id": "_HmX9cmauv2d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Same with the data:\n",
        "strain_H1_32=strain_H1.crop(time_centre - 16,time_centre + 16)"
      ],
      "metadata": {
        "id": "bWlYluXDIHn0"
      },
      "id": "bWlYluXDIHn0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FFT of the data, with the appropriate normalisation\n",
        "data_f=strain_H1_32.average_fft(window=('tukey',1./4.))*(strain_H1_32.duration/2)"
      ],
      "metadata": {
        "id": "2kXYmi2VIKG8"
      },
      "id": "2kXYmi2VIKG8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Cropped data is fourier transformed and normalised with the duration of the signal divided by two.\n",
        "\n",
        "Normalization ensures that the Fourier Transform (FFT) output has the correct physical meaning and units. Without proper normalization, the power spectral density (PSD) or any further analysis based on the FFT would be incorrectly scaled\n"
      ],
      "metadata": {
        "id": "aPeJz8_GIMh6"
      },
      "id": "aPeJz8_GIMh6"
    },
    {
      "cell_type": "code",
      "source": [
        "# FFT of the template, with the appropriate normalisation\n",
        "template_f=template.average_fft(window=('tukey',1./4.))*(template.duration/2)"
      ],
      "metadata": {
        "id": "OoEYVeAgIOjj"
      },
      "id": "OoEYVeAgIOjj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will need the PSD with the same frequency spacing as the data and template,\n",
        "# so we interpolate it to match:\n",
        "Pxx_H1_32=Pxx_H1.interpolate(data_f.df.value)"
      ],
      "metadata": {
        "id": "HORxjz8NIQjt"
      },
      "id": "HORxjz8NIQjt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With the right normalisation, this is equation 7.58 of the textbook:\n",
        "optimal=data_f*template_f.conjugate()/Pxx_H1_32\n",
        "opt_time=2*optimal.ifft()*(optimal.df*2)"
      ],
      "metadata": {
        "id": "u5oS3CW3ISeC"
      },
      "id": "u5oS3CW3ISeC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is equation 7.49 of the textbook: the overlap of the template with itself\n",
        "sigmasq = 4 * np.real((template_f * template_f.conjugate() / Pxx_H1_32).sum() * template_f.df)\n",
        "sigma = np.sqrt(np.abs(sigmasq))\n",
        "\n",
        "# And now we have the SNR time series:\n",
        "SNR_complex = opt_time/sigma"
      ],
      "metadata": {
        "id": "TbeIc1rGIUAn"
      },
      "id": "TbeIc1rGIUAn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can recenter thing with the location of peak in the template:\n",
        "peaksample = template.argmax()\n",
        "SNR_complex = np.roll(SNR_complex,peaksample)\n",
        "SNR = abs(SNR_complex)"
      ],
      "metadata": {
        "id": "DX0Dl7yZIVer"
      },
      "id": "DX0Dl7yZIVer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SNRmax=SNR.max().value\n",
        "time_max=SNR.times[SNR.argmax()]\n",
        "print('Maximum SNR of {} at {}.'.format(SNRmax,time_max))"
      ],
      "metadata": {
        "id": "Yu_9WXsBIXPf"
      },
      "id": "Yu_9WXsBIXPf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(SNR.times,SNR)\n",
        "plt.xlabel('time [s]')\n",
        "plt.ylabel('SNR');"
      ],
      "metadata": {
        "id": "Rx5u9r_VIZCu"
      },
      "id": "Rx5u9r_VIZCu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duration = strain_H1_32.duration.value  # Should be 32 seconds\n",
        "print(duration)"
      ],
      "metadata": {
        "id": "6uxjRUGCIbAO"
      },
      "id": "6uxjRUGCIbAO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = strain_H1_32.dt.value  # Sample spacing"
      ],
      "metadata": {
        "id": "_3ofKrnPIcrw"
      },
      "id": "_3ofKrnPIcrw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = np.arange(-duration / 2, duration / 2, dt)"
      ],
      "metadata": {
        "id": "KYo3djSyIevB"
      },
      "id": "KYo3djSyIevB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"t min: {t.min()}, t max: {t.max()}, dt: {dt}, length: {len(t)}\") #shows the bounds we need for the match filtering at 2048Hz\n"
      ],
      "metadata": {
        "id": "NBXOTuYkIh_v"
      },
      "id": "NBXOTuYkIh_v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gausspulse was not allowing me to generate a waveform with these constraints, so I generated my own waveform.\\"
      ],
      "metadata": {
        "id": "WMdJRP7tIlQ6"
      },
      "id": "WMdJRP7tIlQ6"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the time array (16 seconds centered at 0)\n",
        "t = np.arange(-16, 16, 0.000244140625)\n",
        "\n",
        "# Frequency and standard deviation for the Gaussian\n",
        "fc = 95  # 100 Hz center frequency\n",
        "sigma = 2  # Adjust the Gaussian width\n",
        "\n",
        "# Generate the Gaussian-modulated sine wave\n",
        "gaussian_pulse_manual = np.exp(-t**2 / (2 * sigma**2)) * np.sin(2 * np.pi * fc * t)\n",
        "\n",
        "# Plot the result\n",
        "plt.plot(t, gaussian_pulse_manual)\n",
        "plt.xlabel(\"Time [s]\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.title(\"Manual Gaussian Pulse (100 Hz)\")\n",
        "plt.show()\n",
        "\n",
        "# Check if the pulse has non-zero values\n",
        "print(f\"Max pulse value: {np.max(gaussian_pulse_manual)}, Min: {np.min(gaussian_pulse_manual)}\")"
      ],
      "metadata": {
        "id": "ZqhsA2itIoSS"
      },
      "id": "ZqhsA2itIoSS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pulse is constrained between -16 and 16 seconds, with a dt of 0.000244140625s."
      ],
      "metadata": {
        "id": "oLF2ycctxl26"
      },
      "id": "oLF2ycctxl26"
    },
    {
      "cell_type": "code",
      "source": [
        "from gwpy.timeseries import TimeSeries\n",
        "\n",
        "# Convert to TimeSeries (use the same sample rate as strain_H1)\n",
        "gaussian_pulse_series = TimeSeries(gaussian_pulse_manual, dt=strain_H1.dt.value)\n",
        "gaussian_pulse_series *= 7.6e-19  # Scale up to match GW strain levels\n",
        "\n",
        "\n",
        "#gaussian_pulse_series = np.nan_to_num(gaussian_pulse_series, nan=0.0)\n",
        "\n",
        "\n",
        "#gaussian_pulse_series = np.max(np.abs(gaussian_pulse_series))  # Normalize\n",
        "#gaussian_pulse_series *= 1e21  # Scale up to match GW strain levels\n"
      ],
      "metadata": {
        "id": "eabRMLGwiElQ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eabRMLGwiElQ"
    },
    {
      "cell_type": "code",
      "source": [
        "gaussian_white = gaussian_pulse_series.whiten(fftlength=4, overlap=2, window=('tukey', 1./4.))"
      ],
      "metadata": {
        "id": "jqKO0AsziG7x"
      },
      "execution_count": null,
      "outputs": [],
      "id": "jqKO0AsziG7x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whitens the gausspulse series in the same way previously discussed"
      ],
      "metadata": {
        "id": "Lb-ToCGUxx1q"
      },
      "id": "Lb-ToCGUxx1q"
    },
    {
      "cell_type": "code",
      "source": [
        "gaussian_f = gaussian_white.average_fft(window=('tukey', 1./4.)) * (gaussian_white.duration / 2)\n"
      ],
      "metadata": {
        "id": "Gpdgh8nBjrRQ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Gpdgh8nBjrRQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure PSD matches the frequency resolution\n",
        "Pxx_H1_interp = Pxx_H1.interpolate(gaussian_f.df.value)\n",
        "\n",
        "# Compute the matched filter SNR\n",
        "optimal_gaussian = strain_H1_32.average_fft(window=('tukey', 1./4.)) * gaussian_f.conjugate() / Pxx_H1_interp\n",
        "opt_time_gaussian = 2 * optimal_gaussian.ifft() * (optimal_gaussian.df * 2)\n",
        "\n",
        "# Compute sigma^2 (Equation 7.49)\n",
        "sigma_sq_gaussian = 4 * np.real((gaussian_f * gaussian_f.conjugate() / Pxx_H1_interp).sum() * gaussian_f.df)\n",
        "sigma_gaussian = np.sqrt(np.abs(sigma_sq_gaussian))\n",
        "\n",
        "# Final SNR time series\n",
        "SNR_gaussian = opt_time_gaussian / sigma_gaussian\n"
      ],
      "metadata": {
        "id": "aHKiEj10ju_R"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aHKiEj10ju_R"
    },
    {
      "cell_type": "code",
      "source": [
        "SNRmax_gaussian = SNR_gaussian.max().value\n",
        "time_max_gaussian = SNR_gaussian.times[SNR_gaussian.argmax()]\n",
        "\n",
        "print(f\"Maximum SNR for Gaussian Pulse: {SNRmax_gaussian} at {time_max_gaussian} s.\")\n",
        "\n",
        "plt.plot(SNR_gaussian.times, SNR_gaussian, label=\"SNR (Gaussian Pulse)\")\n",
        "plt.xlabel(\"Time [s]\")\n",
        "plt.ylabel(\"SNR\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PcEvoXC1jxC_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "PcEvoXC1jxC_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SNR calculated using the CBC method was 14.99, which is an appopriate value which indicates a high likelihood of detection (which will be confirmed statistically later). Conversely, the burst method failed in determining any sort of signal within the noise, with a value of 0.22.\n",
        "\n",
        "There are numerous reason for this, one being the allowance of the CBC model to account for parameters intinsic to the system. The prior knowledge of knowing that the signal's origin is a BBH merger, and furthermore the mass and redshift parameters are beneficial in modelling the signal. In comparison, the only parameters which can be altered using the burst method are the central frequency and bandwidth which makes it quite difficult to accurately match filter the signal. The fundamental nature of the signal being a CBC instead of a burst also leads to difficulty, since the sine gaussian does not accurately capture the long terms portions of the signal such as the inspiral phase.\n",
        "\n",
        "To conlcude, the pulse seems to be a far too simplistic model to accurately match filter the signal.\n",
        "\n",
        "The true SNR was ~25. The disrepency may be due to the model chosen for the CBC. NRSur7dq4 seemed to give the most accurate value for the SNR, however I could not get this to work :(."
      ],
      "metadata": {
        "id": "B9sl9I4O4nNo"
      },
      "id": "B9sl9I4O4nNo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing the Methods:\n",
        "\n",
        "    Matched Filtering for CBC (SNR ~ 15):\n",
        "\n",
        "        SNR of 15 is a solid detection, and it's a result of using template waveforms that match the CBC signal well. This is the state-of-the-art technique for detecting known gravitational wave signals, and it works well even in noisy data.\n",
        "\n",
        "    Gaussian Pulse (Low SNR):\n",
        "\n",
        "        A single Gaussian pulse is much simpler and doesn't model the complexity of gravitational waves from compact binaries. The SNR will naturally be low, especially if the Gaussian pulse doesn't match the signal well.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "The Matched Filtering method for CBCs is indeed superior and much more reliable for gravitational wave detection because it uses templates that capture the specific waveform from binary mergers. A SNR of 15 is considered excellent for a CBC detection.\n",
        "\n",
        "In contrast, using a Gaussian pulse is a simplified approximation and doesn't capture the full complexity of CBC signals, which is why the SNR is lower in that case."
      ],
      "metadata": {
        "id": "zfOcnoTMxOGf"
      },
      "id": "zfOcnoTMxOGf"
    },
    {
      "cell_type": "markdown",
      "id": "cardiovascular-yukon",
      "metadata": {
        "id": "cardiovascular-yukon"
      },
      "source": [
        "## Parameter Estimation (25 points)\n",
        "Moving to a **coherent** analysis:\n",
        "- Using the CBC model, build likelihood, prior and posterior functions for the network of gravitational-wave detectors.\n",
        "- Maximise the likelihood function and plot relevant quantities.\n",
        "- Interface the posterior function with a sampling algorithm.\n",
        "    - You are free to select any ready-made algorithm, or write your own.\n",
        "- Perform the sampling, and plot the results.\n",
        "    -  This step can take a long time, and so it is best to develop and test first with a few sampling iterations, and not with the whole sampling run.\n",
        "- Compare the results with the literature and discuss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaptive-birmingham",
      "metadata": {
        "id": "adaptive-birmingham"
      },
      "outputs": [],
      "source": [
        "data={} # an empty dictionary\n",
        "ifos=['L1','H1'] # a list which we use as the dictionary keys\n",
        "for ifo in ['H1','L1']:\n",
        "    data[ifo] = TimeSeries.fetch_open_data(ifo, time_centre - 16, time_centre +16)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the low frequency content and downsample the data to 2048Hz\n",
        "for ifo in ['H1','L1']:\n",
        "    data[ifo] = data[ifo].highpass(15).resample(2048)"
      ],
      "metadata": {
        "id": "RZy4OcrDJIuu"
      },
      "id": "RZy4OcrDJIuu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GOing to down sample for speed. In the paper, for CBC model parameter estimation, the sampling rate was also down sampled from 4096 Hz to 2048 Hz."
      ],
      "metadata": {
        "id": "NAwE3abUPETO"
      },
      "id": "NAwE3abUPETO"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "colours=['gwpy:ligo-livingston','gwpy:ligo-hanford']\n",
        "for ifo,colour in zip(ifos,colours):\n",
        "    plt.plot(data[ifo],label='{} data'.format(ifo),color=colour)\n",
        "plt.xlabel('GPS Time [s]')\n",
        "plt.ylabel('Strain')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "Rzeyl6mtPsFY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Rzeyl6mtPsFY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The waveforms seem fairly similar, with the glitch in L1 as described in the paper being fairly obvious."
      ],
      "metadata": {
        "id": "LaFlo7nuP-5a"
      },
      "id": "LaFlo7nuP-5a"
    },
    {
      "cell_type": "code",
      "source": [
        "psd={}\n",
        "for ifo in ifos:\n",
        "    psd[ifo] = data[ifo].psd(fftlength=4)"
      ],
      "metadata": {
        "id": "ygFDYZG2R2zi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ygFDYZG2R2zi"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zwxi3FlJW-I"
      },
      "id": "8zwxi3FlJW-I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "for ifo,colour in zip(ifos,colours):\n",
        "    plt.loglog(psd[ifo],label='{} PSD'.format(ifo),color=colour)\n",
        "plt.xlabel('Frequency [Hz]')\n",
        "plt.ylabel('Strain noise [$1/\\mathrm{Hz}$]')\n",
        "plt.xlim(20,1024)\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "NunG2dXcSc4_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "NunG2dXcSc4_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "PSD for both detectors. Spectral leakage becomes obvious below approximately ~60Hz due to larger variation in strain for each frequency bin. This occurs due to longer wavelength signals being more affected by the truncation applied by FFT when calculating the PSD. This necessitates band passing, since when whitening the data we want to focus on the regions which best represent the signal."
      ],
      "metadata": {
        "id": "TTUEQnjZ9zTq"
      },
      "id": "TTUEQnjZ9zTq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Waveform systematics are expected to play a significant role in GW200129’s inference (e.g. Refs. [1, 14, 101]), which motivates utilizing NRSur7dq4 for all of our main text results.\n",
        "\n",
        "It does not work :("
      ],
      "metadata": {
        "id": "RbQqB6yMTeoT"
      },
      "id": "RbQqB6yMTeoT"
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = 34.5 * (1 + 0.18)\n",
        "m2 = 29 * (1 + 0.18)\n",
        "hp, hc = get_td_waveform(approximant='SEOBNRv4_opt',\n",
        "                          mass1=m1,\n",
        "                          mass2=m2,\n",
        "                          delta_t=data['H1'].dt.value,\n",
        "                          f_lower=20)\n",
        "plt.plot(hp.sample_times, hp)\n",
        "plt.xlabel('Time (s)');"
      ],
      "metadata": {
        "id": "hFcyH-C4TliP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hFcyH-C4TliP"
    },
    {
      "cell_type": "code",
      "source": [
        "from pycbc.detector import Detector"
      ],
      "metadata": {
        "id": "gB4qyJ6_U_17"
      },
      "execution_count": null,
      "outputs": [],
      "id": "gB4qyJ6_U_17"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the detector objects:\n",
        "det={}\n",
        "for ifo in ifos:\n",
        "    det[ifo]=Detector(ifo)"
      ],
      "metadata": {
        "id": "UOR_P_qxVCyi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "UOR_P_qxVCyi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ifo stands for interferometers, structring the data for the GW event into a parsable format from each detector."
      ],
      "metadata": {
        "id": "s62xSBJq_lp7"
      },
      "id": "s62xSBJq_lp7"
    },
    {
      "cell_type": "code",
      "source": [
        "declination = 0\n",
        "right_ascension = 5.5\n",
        "polarization = 0.0 # uniform prior\n",
        "fp={}\n",
        "fc={}\n",
        "for ifo in ifos:\n",
        "    fp[ifo], fc[ifo] = det[ifo].antenna_pattern(right_ascension, declination,polarization, time_centre)\n",
        "    print(\"{}: fp={}, fc={}\".format(ifo,fp[ifo], fc[ifo]))"
      ],
      "metadata": {
        "id": "-RAqeR4hVFyh"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-RAqeR4hVFyh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "fp and fc are the antenna pattern functions. They are necessary for understanding plus-polsarisation and cross-polarisation responses for the interferometers at the time the signal arrived at both. This will evidently be different as H1 and L1 are at different locations. These parameters are very important in determining how much of both polarisations of the signal were detected by each interferometer, and therefore the GW sigal detected by each detector.\n"
      ],
      "metadata": {
        "id": "SfHV6CObALIC"
      },
      "id": "SfHV6CObALIC"
    },
    {
      "cell_type": "code",
      "source": [
        "ht={}\n",
        "for ifo in ifos:\n",
        "    ht[ifo] = fp[ifo] * hp + fc[ifo] * hc"
      ],
      "metadata": {
        "id": "p2aFkj_hWsZ0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "p2aFkj_hWsZ0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the antenna beam pattern functions we can then compute the gravitational-wave signal as\n",
        "seen by each detector:"
      ],
      "metadata": {
        "id": "UXgiGa8eWz6E"
      },
      "id": "UXgiGa8eWz6E"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "for ifo,colour in zip(ifos,colours):\n",
        "    plt.plot(TimeSeries.from_pycbc(ht[ifo]),label='{} signal'.format(ifo),color=colour)\n",
        "plt.xlabel('GPS Time [s]')\n",
        "plt.ylabel('Strain')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "08O-xiRfW3bs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "08O-xiRfW3bs"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "white_data={}\n",
        "for ifo,colour in zip(ifos,colours):\n",
        "    white_data[ifo]=data[ifo].whiten(fftlength=4).bandpass(60,120)\n",
        "    plt.plot(white_data[ifo],label='{} whitened data'.format(ifo),color=colour)\n",
        "#plt.xlim(time_centre-4, time_centre+5.3)\n",
        "plt.xlim(time_centre-0.4, time_centre+0.2)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "nEQ0z8eKXD_u"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nEQ0z8eKXD_u"
    },
    {
      "cell_type": "code",
      "source": [
        "time_delay=det['H1'].time_delay_from_earth_center(right_ascension, declination,time_centre)\n",
        "print(\"For the sky-position ra={},dec={}, at time={},\".format(right_ascension,declination,time_centre))\n",
        "print(\"the time delay between Hanford and geocenter is {} seconds\".format(time_delay))"
      ],
      "metadata": {
        "id": "7LOZTVnQaTfu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7LOZTVnQaTfu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculates the time delay between each detector."
      ],
      "metadata": {
        "id": "LuYuTZAmBuWc"
      },
      "id": "LuYuTZAmBuWc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in chapter 7, we’ll make a light-weight wrapper for get_td_waveform() that will take parameter\n",
        "vector⃗ 𝜆 and generate a waveform for us with the same sampling rate and time window as our data.\n",
        "However, this time we need to project the “plus” and “cross” polarisations onto the detector arms"
      ],
      "metadata": {
        "id": "NY_WQc-3alQz"
      },
      "id": "NY_WQc-3alQz"
    },
    {
      "cell_type": "code",
      "source": [
        "for ifo in ifos:\n",
        "    data[ifo]=data[ifo].crop(time_centre-2,time_centre+2)"
      ],
      "metadata": {
        "id": "dIgEbiuUae2k"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dIgEbiuUae2k"
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_template(param,\n",
        "    delta_t=data['H1'].dt.value, # Assuming all IFOs have the same dt\n",
        "    duration=data['H1'].duration.value, # Assuming all IFOs have the same duration !\n",
        "    start_time=data['H1'].x0.value,# Assuming all IFOs have the same start time !\n",
        "    f_lower=20.):\n",
        "    m1, m2, distance, time, phase, right_ascension, declination, inclination,polarization = param\n",
        "\n",
        "    hp, hc = get_td_waveform(approximant=\"SEOBNRv4_opt\",\n",
        "                            mass1=m1,\n",
        "                            mass2=m2,\n",
        "                            distance=distance,\n",
        "                            inclination=inclination,\n",
        "                            coa_phase=phase,\n",
        "                            delta_t=delta_t,\n",
        "                            f_lower=f_lower)\n",
        "  # Resize the signal buffer\n",
        "    hp.resize(int(duration/delta_t))\n",
        "    hc.resize(int(duration/delta_t))\n",
        "    ht={}\n",
        "    template={}\n",
        "# compute the detectors responses and shift to the requested time\n",
        "    for ifo in ifos:\n",
        "        fp, fc = det[ifo].antenna_pattern(right_ascension, declination,polarization, time)\n",
        "        ht[ifo] = fp * hp.copy() + fc * hc.copy()\n",
        "        time_delay = det[ifo].time_delay_from_earth_center(right_ascension,declination, time)\n",
        "        ht[ifo] = ht[ifo].cyclic_time_shift(ht[ifo].start_time + time - start_time + time_delay)\n",
        "        ht[ifo].start_time=start_time\n",
        "        template[ifo]=TimeSeries.from_pycbc(ht[ifo])\n",
        "    return template"
      ],
      "metadata": {
        "id": "ovL-g39YapU2"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ovL-g39YapU2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the gen_template() function:\n",
        "param=[34.5,29,890,1264316116.4,0.0,5.5, 0, 0.0, 0.]\n",
        "# m1, m2, distance, time, phase, right_ascension, declination, inclination,polarization = param\n",
        "template=gen_template(param)\n",
        "plt.figure(figsize=(15,5))\n",
        "for ifo,colour in zip(ifos,colours):\n",
        "  plt.plot(template[ifo],label='{} signal'.format(ifo),color=colour)\n",
        "plt.xlabel('GPS Time [s]')\n",
        "plt.ylabel('Strain')\n",
        "plt.legend();\n",
        "plt.axvline(time_centre,c='forestgreen')\n",
        "plt.xlim([time_centre-0.2,time_centre+0.05])"
      ],
      "metadata": {
        "id": "u2KsA8R6cK5I"
      },
      "execution_count": null,
      "outputs": [],
      "id": "u2KsA8R6cK5I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculates the CBC model for each detector with parameters taken from the paper."
      ],
      "metadata": {
        "id": "bV8QoB_9B7IM"
      },
      "id": "bV8QoB_9B7IM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the network waveform generator now defined, we can define our likelihood function, which is\n",
        "equivalent to the product of single-detector likelihood functions\n",
        "\n",
        "To construct a likelihood function for the GW detector network (the two LIGO instruments in this\n",
        "case), we will make the very reasonable assumption that noise is independent between detectors.\n",
        "This means that our network likelihood fuction is just the product of single-detector likelihood\n",
        "functions"
      ],
      "metadata": {
        "id": "S6QKkmpDeNV_"
      },
      "id": "S6QKkmpDeNV_"
    },
    {
      "cell_type": "code",
      "source": [
        "# FFT the data once, ahead of time\n",
        "sf={}\n",
        "for ifo in ifos:\n",
        "    sf[ifo] = data[ifo].average_fft(window=('tukey',1./4.))*data[ifo].duration.value/2\n",
        "def loglikelihood(param, sf=sf, f_lower=20.0):\n",
        "    hf_hp = {}\n",
        "\n",
        "    logl=0.0\n",
        "\n",
        "    template = gen_template(param, delta_t=data['H1'].dt.value ,f_lower=f_lower)\n",
        "\n",
        "    for ifo in ifos:\n",
        "        # zero out the frequencies below f_lower\n",
        "        sf_hp = sf[ifo].crop(start=f_lower)\n",
        "        psd_hp = psd[ifo].crop(start=f_lower)\n",
        "\n",
        "        hf = template[ifo].average_fft(window=('tukey',1./4.))*template[ifo].duration.value/2\n",
        "        hf_hp = hf.crop(start=f_lower)\n",
        "\n",
        "        h_dot_h = 4 * np.real((hf_hp * hf_hp.conjugate() / psd_hp).sum() *hf_hp.df)\n",
        "        h_dot_s = 4 * np.real((sf_hp * hf_hp.conjugate() / psd_hp).sum() *sf_hp.df)\n",
        "\n",
        "        logl += h_dot_s - h_dot_h/2\n",
        "    return logl.value"
      ],
      "metadata": {
        "id": "Dih0MZ4L7IlI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Dih0MZ4L7IlI"
    },
    {
      "cell_type": "code",
      "source": [
        "param0=[34.5,29,890,1264316116.4,0.0,0, 0.0, 0.0, 0.]\n",
        "# m1, m2, distance, time, phase, right_ascension, declination, inclination,polarization = param\n",
        "\n",
        "template0 = gen_template(param0)\n",
        "\n",
        "for ifo in ifos:\n",
        "    white_template = template0[ifo].whiten(asd=np.sqrt(psd[ifo]),highpass=20.)\n",
        "\n",
        "    plt.figure(figsize=[15, 3])\n",
        "    plt.plot(white_data[ifo].times, white_data[ifo], label=\"Data\")\n",
        "    plt.plot(white_template.times, white_template, label=\"Template\")\n",
        "    plt.xlim(time_centre-0.4, time_centre+0.2)\n",
        "    plt.legend();\n",
        "\n",
        "print(loglikelihood(param0))"
      ],
      "metadata": {
        "id": "4KMIoSMo747u"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4KMIoSMo747u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s maximize the likelihood to find the best-fit signal. We’ll use the minimize() function\n",
        "provided by scipy using the Powell method, since it’s pretty good at dealing with non-smooth\n",
        "functions. We’ll also define a callback function to print likelihood values and plot the model as it\n",
        "works."
      ],
      "metadata": {
        "id": "Yu3rOViW8vvh"
      },
      "id": "Yu3rOViW8vvh"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, figsize=[15, 6])\n",
        "Neval = 1\n",
        "\n",
        "lines = {}\n",
        "for ax, ifo in zip(axs, ifos):\n",
        "    hite_template = template0[ifo].whiten(asd=np.sqrt(psd[ifo]),highpass=20.)\n",
        "\n",
        "    ax.plot(white_data[ifo].times, white_data[ifo], label=\"Data\")\n",
        "    lines[ifo], = ax.plot(white_template.times, white_template, label=\"Template\")\n",
        "\n",
        "    ax.set_xlim(time_centre-.4, time_centre+.2)\n",
        "    ax.legend()\n",
        "\n",
        "def callback(param_i):\n",
        "    global Neval\n",
        "    global line\n",
        "    global fig\n",
        "\n",
        "    template = gen_template(param_i)\n",
        "    for ifo in ifos:\n",
        "        white_template = template[ifo].whiten(asd=np.sqrt(psd[ifo]),highpass=20.)\n",
        "        lines[ifo].set_ydata(white_template)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    display(fig)\n",
        "    print(\"Steps\\tlog(likelihood)\")\n",
        "    print('{}\\t{:.3f}'.format(Neval, loglikelihood(param_i)))\n",
        "\n",
        "    Neval += 1\n",
        "\n",
        "res = minimize(lambda param: -loglikelihood(param), param0, callback=callback, method='powell')\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "0kU-GfO8RYWD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0kU-GfO8RYWD"
    },
    {
      "cell_type": "code",
      "source": [
        "best_fit = res.x\n",
        "best_fit_template = gen_template(best_fit) #best fit waveform found from optimisation of likelihood"
      ],
      "metadata": {
        "id": "hMWUoyFchT31"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hMWUoyFchT31"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's subtract it from the data and see how consistent the residuals are with noise. We are using here the Q-transform to produce a high-resolution time-frequency map of the data:"
      ],
      "metadata": {
        "id": "YZnlL-52hjnB"
      },
      "id": "YZnlL-52hjnB"
    },
    {
      "cell_type": "code",
      "source": [
        "for i, ifo in enumerate(ifos):\n",
        "    subtracted = data[ifo] - best_fit_template[ifo]\n",
        "\n",
        "    # Plot the original data and the subtracted signal data\n",
        "    for d, title in [(data[ifo], 'Original {} Data'.format(ifo)),\n",
        "                 (subtracted, 'Signal Subtracted from {} Data'.format(ifo))]:\n",
        "\n",
        "        qspecgram=d.whiten(asd=np.sqrt(psd[ifo])).q_transform(outseg=(time_centre - 1, time_centre + 1),\n",
        "                                                     frange=(20, 512))\n",
        "\n",
        "        plot = qspecgram.plot(figsize=[8, 4],vmin=0,vmax=300)\n",
        "        ax = plot.gca()\n",
        "        ax.set_title(title)\n",
        "        ax.set_xscale('seconds')\n",
        "        ax.set_yscale('log')\n",
        "        ax.set_ylim(20, 500)\n",
        "        ax.set_ylabel('Frequency [Hz]')\n",
        "        ax.grid(True, axis='y', which='both')\n",
        "        ax.colorbar(cmap='viridis', label='Normalized energy')"
      ],
      "metadata": {
        "id": "6kLKFrgvhWtI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6kLKFrgvhWtI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see above the generated waveform is successful as the residual between the data and generated waveform appears to be negligable. This indicates that the match filtering was succesful. Furthermore, it shows the discrepency between SNR in the H1 and L1 detectors."
      ],
      "metadata": {
        "id": "4hlOcHCjh0UN"
      },
      "id": "4hlOcHCjh0UN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's make a model without any known priors (from the paper). We will assume a uniform prior since we don't know anything about the signal in this case."
      ],
      "metadata": {
        "id": "TzPxmPyjh89o"
      },
      "id": "TzPxmPyjh89o"
    },
    {
      "cell_type": "code",
      "source": [
        "param0=[34.5,29,890,1264316116.4,0.0,0, 0.0, 0.001, 0.]\n",
        "\n",
        "param=[34.5,29,890,1264316116.4,0.0,5.5, 0, 0.001, 0.]\n"
      ],
      "metadata": {
        "id": "9CNL4dYqgY09"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9CNL4dYqgY09"
    },
    {
      "cell_type": "code",
      "source": [
        "def logprior(param):\n",
        "    # Unpack parameters\n",
        "    m1, m2, distance, time, phase, ra, dec, inclination, polarization = param\n",
        "\n",
        "    # Enforce ordering of masses\n",
        "    if m1 < m2 or m1 <= 5 or m2 <= 5:  # Ensure m1 >= m2 and reasonable limits\n",
        "        return -np.inf\n",
        "\n",
        "    # Apply Gaussian prior on masses (adjust mean & std as needed)\n",
        "    mu_m1, sigma_m1 = 30, 15  # Mean and std for m1\n",
        "    mu_m2, sigma_m2 = 20, 12  # Mean and std for m2\n",
        "    logprior_m1 = -0.5 * ((m1 - mu_m1) / sigma_m1) ** 2\n",
        "    logprior_m2 = -0.5 * ((m2 - mu_m2) / sigma_m2) ** 2\n",
        "\n",
        "    # Apply inclination prior (sin distribution)\n",
        "    logprior_inclination = np.log(np.sin(inclination)) if 0 < inclination < np.pi else -np.inf\n",
        "\n",
        "    # Apply power-law prior for distance\n",
        "    d_max = 5000  # Max reasonable distance in Mpc\n",
        "    if distance > d_max or distance <= 0:\n",
        "        return -np.inf\n",
        "    logprior_distance = -((distance - d_max) / 500) ** 2 if distance > d_max else 0  # Smooth cutoff\n",
        "\n",
        "    # Check valid ranges for angles\n",
        "    for angle in [ra, phase, polarization]:\n",
        "        if not (0 <= angle <= 2 * np.pi):\n",
        "            return -np.inf\n",
        "\n",
        "    if not (-np.pi / 2 <= dec <= np.pi / 2):\n",
        "        return -np.inf\n",
        "\n",
        "    # Sum all priors\n",
        "    return logprior_m1 + logprior_m2 + logprior_inclination + logprior_distance\n"
      ],
      "metadata": {
        "id": "y4pWDFN7jhak"
      },
      "execution_count": null,
      "outputs": [],
      "id": "y4pWDFN7jhak"
    },
    {
      "cell_type": "markdown",
      "source": [
        "With network likelihood and prior defined we can now take their product to get the posterior."
      ],
      "metadata": {
        "id": "-BGcnnqxiR5d"
      },
      "id": "-BGcnnqxiR5d"
    },
    {
      "cell_type": "code",
      "source": [
        "def logposterior(param):\n",
        "    logprior_val = logprior(param)\n",
        "\n",
        "    # Reject invalid parameter values immediately\n",
        "    if logprior_val == -np.inf:\n",
        "        return -np.inf  # Reject sample\n",
        "\n",
        "    loglikelihood_val = loglikelihood(param)\n",
        "\n",
        "    # Check for NaNs in likelihood (e.g., numerical issues)\n",
        "    if np.isnan(loglikelihood_val):\n",
        "        return -np.inf\n",
        "\n",
        "    return logprior_val + loglikelihood_val"
      ],
      "metadata": {
        "id": "Y-lWrPUgiTk9"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Y-lWrPUgiTk9"
    },
    {
      "cell_type": "code",
      "source": [
        "logposterior(param0)"
      ],
      "metadata": {
        "id": "sn_fGAhNiV15"
      },
      "execution_count": null,
      "outputs": [],
      "id": "sn_fGAhNiV15"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We can now maximize the posterior to determine the maximum a posteriori (MAP) parameters.\n",
        "\n",
        "Where the maximum likelihood point was the \"best fit\" in the sense that it found the parameters that produced a model most consistent with the data, the MAP is more of a \"best guess\" in that it balances fitting the data with being consistent with prior expectations for source parameters.\n",
        "\n",
        "However, as the posterior is a density, the MAP does depend on the parametrisation we chose.\n"
      ],
      "metadata": {
        "id": "7gSkjVKmijMF"
      },
      "id": "7gSkjVKmijMF"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, figsize=[15, 6])\n",
        "Neval = 1\n",
        "\n",
        "lines = {}\n",
        "for ax, ifo in zip(axs, ifos):\n",
        "    hite_template = template0[ifo].whiten(asd=np.sqrt(psd[ifo]),highpass=20.)\n",
        "\n",
        "    ax.plot(white_data[ifo].times, white_data[ifo], label=\"Data\")\n",
        "    lines[ifo], = ax.plot(white_template.times, white_template, label=\"Template\")\n",
        "\n",
        "    ax.set_xlim(time_centre-.4, time_centre+.2)\n",
        "    ax.legend()\n",
        "\n",
        "def callback(param_i):\n",
        "    global Neval\n",
        "    global line\n",
        "    global fig\n",
        "\n",
        "    template = gen_template(param_i)\n",
        "    for ifo in ifos:\n",
        "        white_template = template[ifo].whiten(asd=np.sqrt(psd[ifo]),highpass=20.)\n",
        "        lines[ifo].set_ydata(white_template)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    display(fig)\n",
        "    print(\"Steps\\tlog(posterior)\")\n",
        "    print('{}\\t{:.3f}'.format(Neval, logposterior(param_i)))\n",
        "\n",
        "    Neval += 1\n",
        "\n",
        "res = minimize(lambda param: -logposterior(param), param0, callback=callback, method='powell')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "AfN1lY2QilV_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AfN1lY2QilV_"
    },
    {
      "cell_type": "code",
      "source": [
        "best_guess = res.x\n",
        "print(best_guess)"
      ],
      "metadata": {
        "id": "1IJezMv2zlEE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1IJezMv2zlEE"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emcee\n"
      ],
      "metadata": {
        "id": "HZXj62JzRD6Y"
      },
      "execution_count": null,
      "outputs": [],
      "id": "HZXj62JzRD6Y"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install corner"
      ],
      "metadata": {
        "id": "bD0VjSftRVnO"
      },
      "execution_count": null,
      "outputs": [],
      "id": "bD0VjSftRVnO"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import emcee\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import corner\n",
        "\n",
        "\n",
        "# Number of dimensions (parameters)\n",
        "ndim = len(param0)  # Assuming param0 is your initial guess\n",
        "\n",
        "# Number of walkers (chains) - typically 2-3 times ndim\n",
        "nwalkers = 2 * ndim\n",
        "\n",
        "# Initialize walkers near the previous best-fit (add small noise)\n",
        "p0 = [param0 + 1e-4 * np.random.randn(ndim) for i in range(nwalkers)]\n",
        "\n",
        "# Set up the MCMC sampler\n",
        "sampler = emcee.EnsembleSampler(nwalkers, ndim, logposterior)\n",
        "\n",
        "# Run the MCMC chain\n",
        "nsteps = 2000  # Increase if needed\n",
        "sampler.run_mcmc(p0, nsteps, progress=True)\n",
        "\n",
        "# Extract samples\n",
        "tau = sampler.get_autocorr_time()\n",
        "discard = int(2 * np.max(tau))  # Burn-in: discard ~2x the longest autocorr time\n",
        "thin = int(0.5 * np.min(tau))   # Thin every ~half of the shortest autocorr time\n",
        "samples = sampler.get_chain(discard=discard, thin=thin, flat=True)\n",
        "\n",
        "\n",
        "# Plot corner plot to visualize parameter distributions\n",
        "fig = corner.corner(samples, labels=[\"m1\", \"m2\", \"distance\", \"time\", \"phase\", \"ra\", \"dec\", \"inc\", \"pol\"])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bNfX3NTrO6ji"
      },
      "execution_count": null,
      "outputs": [],
      "id": "bNfX3NTrO6ji"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each diagonal plot in a corner plot shows the marginalized 1D posterior distribution for a parameter, while the rest shows the correlation between each parameter."
      ],
      "metadata": {
        "id": "NHjzQIf3YJr_"
      },
      "id": "NHjzQIf3YJr_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A corner plot for a detected binary black hole merger might show:\n",
        "\n",
        "    M1,M2M1​,M2​ (component masses) → Highly correlated.\n",
        "\n",
        "    χeffχeff​ (effective spin) → A broad posterior if spins are poorly constrained.\n",
        "\n",
        "    dLdL​ (luminosity distance) vs. inclination → Shows an anti-correlation."
      ],
      "metadata": {
        "id": "b2QOSeWdZIhn"
      },
      "id": "b2QOSeWdZIhn"
    },
    {
      "cell_type": "code",
      "source": [
        "samples = sampler.get_chain()\n",
        "\n",
        "# Plot chains for each parameter\n",
        "fig, axes = plt.subplots(ndim, figsize=(10, ndim * 2), sharex=True)\n",
        "labels = [\"m1\", \"m2\", \"distance\", \"time\", \"phase\", \"ra\", \"dec\", \"inc\", \"pol\"]\n",
        "\n",
        "for i in range(ndim):\n",
        "    axes[i].plot(samples[:, :, i], alpha=0.5)\n",
        "    axes[i].set_ylabel(labels[i])\n",
        "\n",
        "axes[-1].set_xlabel(\"Step number\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-I5emf2HZmoM"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-I5emf2HZmoM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above shows the parameter estimation performed by the MCMC. This method is a lengthier but more accurate way of parameter estimation than using optimisation. This is due to the fact that the MCMC effectively explores the entire parameter space through its random walk cycle, which gives it the ability to provide a full Baysian posterior distribution. This allows for computation of confidence intervals as opposed to the MAP provided by the optimiser, which is a single value instead of a distribution."
      ],
      "metadata": {
        "id": "NaBZZbpjW67q"
      },
      "id": "NaBZZbpjW67q"
    },
    {
      "cell_type": "markdown",
      "source": [
        " Good (Converged) Trace Plot:\n",
        "\n",
        "    Walkers mix well (i.e., they move up and down and explore the full range of the parameter space).\n",
        "\n",
        "    No long-term trends (e.g., no drift toward higher or lower values).\n",
        "\n",
        "    The chains settle into a stable region after an initial burn-in phase.\n",
        "\n",
        "🔴 Bad (Non-Converged) Trace Plot:\n",
        "\n",
        "    Chains are stuck in one region (poor mixing).\n",
        "\n",
        "    Walkers show a strong drift over time.\n",
        "\n",
        "    Large jumps between steps (suggests the step size might be too big).\n",
        "\n",
        "    Some walkers are far from others, meaning different chains are exploring separate parts of parameter space (indicating poor sampling)."
      ],
      "metadata": {
        "id": "zPG9QWTGaEde"
      },
      "id": "zPG9QWTGaEde"
    },
    {
      "cell_type": "markdown",
      "id": "referenced-rehabilitation",
      "metadata": {
        "id": "referenced-rehabilitation"
      },
      "source": [
        "## Conclusions (15 points)\n",
        "Minimum 300 words.\n",
        "- Discuss your results in the context of the literature.\n",
        "- Mention how your work compares with published results.\n",
        "- Discuss some of the interpretation possibles of GW200129, including relevant citations from the literature, and summarising them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "higher-drunk",
      "metadata": {
        "id": "higher-drunk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}